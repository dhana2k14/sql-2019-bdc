{
    "metadata": {
        "kernelspec": {
            "name": "pysparkkernel",
            "display_name": "PySpark",
            "language": ""
        },
        "language_info": {
            "name": "pyspark",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "# read data from HDFS \r\n",
                "# define data path\r\n",
                "\r\n",
                "data_file = '/COE_Demo/po_data_sample/part-00000-a17142b8-efc2-4e92-8e65-b6ec69ae9ca6-c000.csv'\r\n",
                "df = spark.read.format('csv').options(header = 'true', inferSchema = 'true', ignoreLeadingWhiteSpace = 'true', ignoreTrailingWhiteSpace = 'true').load(data_file)"
            ],
            "metadata": {
                "azdata_cell_guid": "ea3edb55-5ab3-4399-931d-90e730b4895a"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "594e80844fa447c7b0110d48d38adce8"
                        }
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": [
                "# load hdfs data into sql server using spark \r\n",
                "# Connection String \r\n",
                "\r\n",
                "servername = \"jdbc:sqlserver://master-0.master-svc\"\r\n",
                "dbname = \"COE\"\r\n",
                "url = servername + \";\" + \"databaseName=\" + dbname + \";\"\r\n",
                "print(url)\r\n",
                "\r\n",
                "dbtable = 'demo.demo_table_by_spark'\r\n",
                "user = \"bdcadmin\"\r\n",
                "password = \"Admin@@123\"\r\n",
                "\r\n",
                "try:\r\n",
                "    df.write.format('jdbc').mode('overwrite').option('url', url).option('dbtable', dbtable).option('user', user).option('password', password).save()\r\n",
                "except ValueError as error:\r\n",
                "    print(\"JDBC Write failed\", error)\r\n",
                "\r\n",
                "print(\"JDBC write is done!\")"
            ],
            "metadata": {
                "azdata_cell_guid": "399eb826-12ce-47cf-a0a8-e3ca4b8b0f62"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}