{
    "metadata": {
        "kernelspec": {
            "name": "pysparkkernel",
            "display_name": "PySpark"
        },
        "language_info": {
            "name": "pyspark",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Usecase 1: Sentiment Analysis of L&T ECC Vendors\n",
                "\n",
                "## Dealing with Semi-Structured Data stored in HDFS of SQL 2019\n",
                "\n",
                "-   In this notebook we will see how to process, transform, prepare JSON file data for model scoring and score each news items for sentiment labels based on external REST API requests.\n",
                "-   The model end point as REST API is developed outside of SQL 2019 BDC and hosted in Azure for batch and live model scores."
            ],
            "metadata": {
                "azdata_cell_guid": "64ae59da-5db1-4a42-90a3-b863568810d4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql import SparkSession\r\n",
                "spark = SparkSession\\\r\n",
                "        .builder\\\r\n",
                "        .appName(\"Spark_Ingestion_Job\")\\\r\n",
                "        .config(\"spark.executor.memory\", \"20g\")\\\r\n",
                "        .config(\"spark.executor.instances\", \"3\")\\\r\n",
                "        .config(\"spark.master\", \"yarn\")\\\r\n",
                "        .config(\"spark.submit.deployMode\", \"client\")\\\r\n",
                "        .config(\"spark.driver.memory\", \"30g\")\\\r\n",
                "        .enableHiveSupport()\\\r\n",
                "        .getOrCreate()"
            ],
            "metadata": {
                "azdata_cell_guid": "47ad57d0-519f-4940-a902-b4e14f1fa588"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Starting Spark application\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>36</td><td>application_1601979426726_0043</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://172.31.61.93:30443/gateway/default/yarn/proxy/application_1601979426726_0043/\">Link</a></td><td><a target=\"_blank\" href=\"https://172.31.61.93:30443/gateway/default/yarn/container/container_1601979426726_0043_01_000001/root\">Link</a></td><td>✔</td></tr></table>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "40f88ec5ff2049038d1487ae5b7f7056"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "SparkSession available as 'spark'.\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "6db475d8cabc47ccad622fd6a03e7758"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": [
                "###  Loading Data"
            ],
            "metadata": {
                "azdata_cell_guid": "5334480b-7cb3-4ff7-94aa-95dba634f381"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# load json into spark RDD \r\n",
                "news_data_rdd = sc.textFile('/COE/news_data/contify_insights_new.json')\r\n",
                "news_data_df = spark.read.option(\"multiline\", \"true\").json(news_data_rdd)\r\n",
                "news_data_df.show(2)\r\n",
                "print(type(news_data_df))\r\n",
                "print(news_data_df.dtypes)"
            ],
            "metadata": {
                "azdata_cell_guid": "2b23d362-8073-4212-8d7d-ac740bec2953",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# do with RDD\r\n",
                "import json\r\n",
                "news_data_rdd = sc.textFile('/COE/news_data/contify_insights_new.json').map(json.loads)\r\n",
                "news_data_rdd.take(2)"
            ],
            "metadata": {
                "azdata_cell_guid": "595cff00-1a3a-4ddb-8b76-b57bb0bec62c"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "7d67a8e6edc64e34ac8b06cdeb6d1706"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "[{'results': [{'source': {'id': '1829', 'name': 'Domain-b', 'rank': 789288}, 'previews': [], 'url': 'https://www.domain-b.com/companies/companies_s/Siemens/20200824_acquisition.html', 'attachments': [], 'duplicates': [], 'content_types': [{'id': 3, 'name': 'News Articles'}], 'language': {'id': 'en', 'name': 'English'}, 'channel': 'News and Other Websites', 'summary': 'Siemens gets CCI nod for proposed acquisition of C&S Electric The Competition Commission of India (CCI) has approved the proposed acquisition of C&S Electric Limited by Siemens Limited. The combination envisages acquisition of 100 per cent acquisition of the share capital of C&S Electric Limited by Siemens India. At the time of closing of the proposed combination, the scope of business of C&S shall include low-voltage (LV) switchgear components and panels, LV and medium voltage (MV) power busbars as well as protection and metering devices of C&S. Certain other businesses of C&S, such as MV switchgear and package sub-station, lighting, diesel generating sets, engineering, procurement and construction business and the “Etacom” busbars business will be retained by the existing promoters of C&S. Siemens India focuses on the areas of power generation and distribution, intelligent infrastructure for buildings and distributed energy systems, and automation and digitalization in the process and manufacturing industries. It also supplies smart mobility solutions for rail and road transport and infrastructure solutions for Smart Cities. C&S manufactures a range of electrical switchgear, power protection and electrical distribution products. It also has an electrical contracting business which offers turnkey solutions for industrial and commercial electrification, substations and power plants. In addition, C&S is also engaged in the design and execution of grid connected solar photo-voltaic power plants. CCI said it would issue a detailed order in due course.', 'industries': [{'id': 112, 'name': 'Power'}, {'id': 69, 'name': 'Capital Goods'}], 'id': 20082425521615, 'pub_date': '2020-08-23T08:36:14Z', 'custom_topics': [], 'companies': [{'id': 627027, 'url': 'https://cselectric.co.in/', 'logo': '//112233.contify.com/https://112233.contify.com/images/watchlist/company-ico.png?v=3', 'name': 'C&S Electric Limited'}, {'id': 1390, 'url': 'https://new.siemens.com', 'logo': '//112233.contify.com/images/company/logo/siemens-1-29475.png', 'name': 'Siemens AG'}], 'locations': [{'id': 3, 'name': 'India'}], 'title': 'Siemens gets CCI nod for proposed acquisition of C&S Electric', 'image_url': '', 'topics': [{'id': 2497, 'name': 'M&A Activities'}, {'id': 3154, 'name': 'Regulatory and Legal'}]}], 'search_company': 'C&S ELECTRIC LIMITED ', 'next': None, 'previous': None, 'count': 1}, {'results': [{'id': 20082425521616, 'title': 'ESSAR FORGINGS', 'summary': 'Manufacturers & Supplier of forged items, especially for high & low tension transmission line towers like Hangers, D Shackles Anchor Bolts/Foundation Bolts, Earthing Pipe Sets, Anti Climbing Devices, Bird Guards.'}], 'search_company': 'ESSAR FORGINGS', 'next': None, 'previous': None, 'count': 1}]",
                    "output_type": "stream"
                }
            ],
            "execution_count": 47
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql.types import Row \r\n",
                "\r\n",
                "def spliter(lines):\r\n",
                "    data = {}\r\n",
                "    line = lines['results']\r\n",
                "    for d in line:\r\n",
                "        data['id'] = d['id']\r\n",
                "        data['title'] = d['title']\r\n",
                "        data['summary'] = d['summary']\r\n",
                "    data['search_company'] = lines['search_company']\r\n",
                "    return data\r\n",
                "\r\n",
                "rdd_df = news_data_rdd.map(lambda x: Row(**spliter(x))).toDF()\r\n",
                "rdd_df.show(1)"
            ],
            "metadata": {
                "azdata_cell_guid": "c36e7381-2e93-4299-a12f-af994be9a113",
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "b3973b35c1594c1c9003cba34085f67e"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "+--------------+--------------------+--------------------+--------------------+\n|            id|      search_company|             summary|               title|\n+--------------+--------------------+--------------------+--------------------+\n|20082425521615|C&S ELECTRIC LIMI...|Siemens gets CCI ...|Siemens gets CCI ...|\n+--------------+--------------------+--------------------+--------------------+\nonly showing top 1 row",
                    "output_type": "stream"
                }
            ],
            "execution_count": 84
        },
        {
            "cell_type": "code",
            "source": [
                "rdd_df.write.format('csv').mode('overwrite').option('header', True).save('/COE/news_data/rdd_to_df_sample.csv')"
            ],
            "metadata": {
                "azdata_cell_guid": "7426b7f6-00f2-4dff-9d4e-3df560ff718f"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "4f39284c02fa4b978a6c32b9e8897deb"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "text": "An error was encountered:\nAn error occurred while calling o1726.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:677)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:677)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:677)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:286)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:272)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:230)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 75.0 failed 4 times, most recent failure: Lost task 1.3 in stage 75.0 (TID 154, storage-0-1.storage-0-svc.mssql-cluster.svc.cluster.local, executor 1): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4 fields are required while 1 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15$$anonfun$apply$15.applyOrElse(EvaluatePython.scala:184)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.org$apache$spark$sql$execution$python$EvaluatePython$$nullSafeConvert(EvaluatePython.scala:208)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15.apply(EvaluatePython.scala:180)\n\tat org.apache.spark.sql.SparkSession$$anonfun$6$$anonfun$apply$5.apply(SparkSession.scala:752)\n\tat org.apache.spark.sql.SparkSession$$anonfun$6$$anonfun$apply$5.apply(SparkSession.scala:752)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:245)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2065)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4 fields are required while 1 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15$$anonfun$apply$15.applyOrElse(EvaluatePython.scala:184)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.org$apache$spark$sql$execution$python$EvaluatePython$$nullSafeConvert(EvaluatePython.scala:208)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15.apply(EvaluatePython.scala:180)\n\tat org.apache.spark.sql.SparkSession$$anonfun$6$$anonfun$apply$5.apply(SparkSession.scala:752)\n\tat org.apache.spark.sql.SparkSession$$anonfun$6$$anonfun$apply$5.apply(SparkSession.scala:752)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:245)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\n\nTraceback (most recent call last):\n  File \"/tmp/nm-local-dir/usercache/root/appcache/application_1601979426726_0045/container_1601979426726_0045_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 739, in save\n    self._jwrite.save(path)\n  File \"/tmp/nm-local-dir/usercache/root/appcache/application_1601979426726_0045/container_1601979426726_0045_01_000001/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/tmp/nm-local-dir/usercache/root/appcache/application_1601979426726_0045/container_1601979426726_0045_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/tmp/nm-local-dir/usercache/root/appcache/application_1601979426726_0045/container_1601979426726_0045_01_000001/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o1726.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:677)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:677)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:677)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:286)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:272)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:230)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 75.0 failed 4 times, most recent failure: Lost task 1.3 in stage 75.0 (TID 154, storage-0-1.storage-0-svc.mssql-cluster.svc.cluster.local, executor 1): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4 fields are required while 1 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15$$anonfun$apply$15.applyOrElse(EvaluatePython.scala:184)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.org$apache$spark$sql$execution$python$EvaluatePython$$nullSafeConvert(EvaluatePython.scala:208)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15.apply(EvaluatePython.scala:180)\n\tat org.apache.spark.sql.SparkSession$$anonfun$6$$anonfun$apply$5.apply(SparkSession.scala:752)\n\tat org.apache.spark.sql.SparkSession$$anonfun$6$$anonfun$apply$5.apply(SparkSession.scala:752)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:245)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2065)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4 fields are required while 1 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15$$anonfun$apply$15.applyOrElse(EvaluatePython.scala:184)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.org$apache$spark$sql$execution$python$EvaluatePython$$nullSafeConvert(EvaluatePython.scala:208)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15.apply(EvaluatePython.scala:180)\n\tat org.apache.spark.sql.SparkSession$$anonfun$6$$anonfun$apply$5.apply(SparkSession.scala:752)\n\tat org.apache.spark.sql.SparkSession$$anonfun$6$$anonfun$apply$5.apply(SparkSession.scala:752)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:245)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\n\n\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 78
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Data Pre-processing"
            ],
            "metadata": {
                "azdata_cell_guid": "547cdcd4-b09d-4969-9de2-af9a10e04d51"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# import libraries \r\n",
                "import json\r\n",
                "import requests\r\n",
                "import re\r\n",
                "from pyspark.sql import SparkSession, Row\r\n",
                "from pyspark.sql.functions import explode, col, array_contains, udf, lit, encode, regexp_replace\r\n",
                "import pyspark.sql.functions as sf\r\n",
                "import pyspark.sql.types as T\r\n",
                "from pyspark.sql.functions import concat, first\r\n",
                "from pyspark.sql.types import StructType, StructField, StringType, MapType, FloatType, ArrayType, DoubleType"
            ],
            "metadata": {
                "azdata_cell_guid": "39a21f58-2113-4aa7-9ff3-d43860caaf15",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "cd7d3d4f056241eeb3f9aba8b7f5d4e1"
                        }
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": [
                "# view schema \r\n",
                "print(news_data_df.dtypes)\r\n",
                "print(news_data_df.printSchema())"
            ],
            "metadata": {
                "azdata_cell_guid": "98091f2e-cb42-42e2-a0fb-ddceed567ca4"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "fe3e824c5e994bee9de735c8f480f3e9"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "[('count', 'bigint'), ('next', 'string'), ('previous', 'string'), ('results', 'array<struct<attachments:array<string>,channel:string,companies:array<struct<id:bigint,logo:string,name:string,url:string>>,content_types:array<struct<id:bigint,name:string>>,custom_topics:array<string>,duplicates:array<string>,id:bigint,image_url:string,industries:array<struct<id:bigint,name:string>>,language:struct<id:string,name:string>,locations:array<struct<id:bigint,name:string>>,previews:array<string>,pub_date:string,source:struct<id:string,name:string,rank:bigint>,summary:string,title:string,topics:array<struct<id:bigint,name:string>>,url:string>>'), ('search_company', 'string')]\nroot\n |-- count: long (nullable = true)\n |-- next: string (nullable = true)\n |-- previous: string (nullable = true)\n |-- results: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- attachments: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- channel: string (nullable = true)\n |    |    |-- companies: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- logo: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- content_types: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |-- custom_topics: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- duplicates: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- image_url: string (nullable = true)\n |    |    |-- industries: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |-- language: struct (nullable = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |-- locations: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |-- previews: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- pub_date: string (nullable = true)\n |    |    |-- source: struct (nullable = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- rank: long (nullable = true)\n |    |    |-- summary: string (nullable = true)\n |    |    |-- title: string (nullable = true)\n |    |    |-- topics: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |-- search_company: string (nullable = true)\n\nNone"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "code",
            "source": [
                "# # example func\r\n",
                "# def extract_from_list(list_input):\r\n",
                "#     # lt = [item for l in list_input for item in l]\r\n",
                "#     # return(lt)\r\n",
                "#     for item in list_input:\r\n",
                "#         return(item.items())\r\n",
                "\r\n",
                "# # create UDF\r\n",
                "# from pyspark.sql.functions import udf \r\n",
                "# from pyspark.sql.types import ArrayType \r\n",
                "# udf_list_func = udf(lambda x: extract_from_list(x), ArrayType(StringType()))\r\n",
                "\r\n",
                "# # test function with example\r\n",
                "# extract_from_list([{'id': 627027, 'url': 'https: //cselectric.co.in/', 'logo': '//112233.contify.com/https://112233.contify.com/images/watchlist/company-ico.png?v=3', 'name': 'C&S Electric Limited'},\r\n",
                "# \t\t\t\t\t\t  {'id': 1390, 'url': 'https: //new.siemens.com', 'logo': '//112233.contify.com/images/company/logo/siemens-1-29475.png', 'name': 'Siemens AG'}])\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "90abda96-9fd2-4966-b1eb-d83762e94808",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "7b90d58d161b4868885f756a7acd8f5a"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "dict_items([('id', 627027), ('url', 'https: //cselectric.co.in/'), ('logo', '//112233.contify.com/https://112233.contify.com/images/watchlist/company-ico.png?v=3'), ('name', 'C&S Electric Limited')])"
                }
            ],
            "execution_count": 78
        },
        {
            "cell_type": "code",
            "source": [
                "# select records of interest \r\n",
                "df_new = news_data_df.select(col('results.id'), col('results.title'), col('results.url'), col('results.summary'), col('search_company'))\r\n",
                "df_new = df_new.filter(sf.size('url') > 0)\r\n",
                "df_new.show(2)"
            ],
            "metadata": {
                "azdata_cell_guid": "afa2a839-15ab-48bc-9b0c-a4ae91916e5c",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "43d66eab084f43da93a04efff3746ef5"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+----------------+--------------------+--------------------+--------------------+--------------------+\n|              id|               title|                 url|             summary|      search_company|\n+----------------+--------------------+--------------------+--------------------+--------------------+\n|[20082425521615]|[Siemens gets CCI...|[https://www.doma...|[Siemens gets CCI...|C&S ELECTRIC LIMI...|\n|[20082425521616]|    [ESSAR FORGINGS]|                  []|[Manufacturers & ...|      ESSAR FORGINGS|\n+----------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 2 rows"
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "code",
            "source": [
                "# prepare input data for model scoring \r\n",
                "# encode text input\r\n",
                "text_udf = udf(lambda x: \"\".join(x))\r\n",
                "id_udf = udf(lambda x: x[0])\r\n",
                "\r\n",
                "df_new = df_new.withColumn(\"summary\", text_udf(col(\"summary\")))\r\n",
                "df_new = df_new.withColumn(\"id\", id_udf(col(\"id\")))\r\n",
                "df_new = df_new.withColumn(\"title\", id_udf(col(\"title\")))\r\n",
                "df_new = df_new.withColumn(\"url\", id_udf(col(\"url\")))\r\n",
                "# df_new = df_new.withColumn('summary_new', regex_udf(col('summary')))\r\n",
                "df_new = df_new.withColumn(\"summary_new\", regexp_replace(\"summary\", '(\\“)|(\\”)', ''))\r\n",
                "df_new.show(2)\r\n",
                "\r\n",
                "# encode text input\r\n",
                "encode_udf = udf(lambda x: x.encode('utf-8').decode('latin-1'), StringType())\r\n",
                "df_new = df_new.withColumn(\"summary_new\", encode_udf(col(\"summary_new\")))\r\n",
                "df_new.show(2)\r\n",
                "\r\n",
                "#construct text input for model scoring \r\n",
                "df_new_model = df_new.withColumn('model_input_text', concat(sf.lit('{\"news\":\"'), col('summary_new'), sf.lit('\",'), sf.lit('\"name\":\"'), col('search_company'), sf.lit('\"}')))\r\n",
                "df_new_model.show(2)\r\n",
                "\r\n",
                "# view full details of a column\r\n",
                "df_new_model.select('model_input_text').show(2)\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "fd46e0ca-f432-4e80-8adb-9787542a5fa1"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "ecafc3afc00f4f548811cd523f97da2c"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|            id|               title|                 url|             summary|      search_company|         summary_new|\n+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|20082425521615|Siemens gets CCI ...|https://www.domai...|Siemens gets CCI ...|C&S ELECTRIC LIMI...|Siemens gets CCI ...|\n|20082425521616|      ESSAR FORGINGS|                null|Manufacturers & S...|      ESSAR FORGINGS|Manufacturers & S...|\n+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 2 rows\n\n+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|            id|               title|                 url|             summary|      search_company|         summary_new|\n+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|20082425521615|Siemens gets CCI ...|https://www.domai...|Siemens gets CCI ...|C&S ELECTRIC LIMI...|Siemens gets CCI ...|\n|20082425521616|      ESSAR FORGINGS|                null|Manufacturers & S...|      ESSAR FORGINGS|Manufacturers & S...|\n+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 2 rows\n\n+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|            id|               title|                 url|             summary|      search_company|         summary_new|    model_input_text|\n+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|20082425521615|Siemens gets CCI ...|https://www.domai...|Siemens gets CCI ...|C&S ELECTRIC LIMI...|Siemens gets CCI ...|{\"news\":\"Siemens ...|\n|20082425521616|      ESSAR FORGINGS|                null|Manufacturers & S...|      ESSAR FORGINGS|Manufacturers & S...|{\"news\":\"Manufact...|\n+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 2 rows\n\n+--------------------+\n|    model_input_text|\n+--------------------+\n|{\"news\":\"Siemens ...|\n|{\"news\":\"Manufact...|\n+--------------------+\nonly showing top 2 rows"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": [
                "###  Model Scoring"
            ],
            "metadata": {
                "azdata_cell_guid": "39b69bf9-1af1-437c-83da-72a89125503a"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# sentiment analysis model scoring via Rest API\r\n",
                "#\r\n",
                "\r\n",
                "def sentiment_scores(text_input):\r\n",
                "    response = requests.post(\"http://52.187.124.32:80/api/v1/service/absa-sentiment-predictor-v2/score\", text_input, headers = {'Content-Type' : 'application/json', 'Authorization': 'Bearer 1Q7d5p2SqViNlQbhe6gtHBAiZ5MB58rU'})\r\n",
                "    response = response.json()\r\n",
                "    polarity = response['_doc_polarity']\r\n",
                "    scores = response['scores']\r\n",
                "    return(polarity, scores)"
            ],
            "metadata": {
                "azdata_cell_guid": "8e236992-4522-4490-b5cf-ea9b45d05e0f"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "b729cf873e8f44e08a44e4dc266b2da1"
                        }
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": [
                "# construct udf \r\n",
                "#\r\n",
                "\r\n",
                "# define schema \r\n",
                "#\r\n",
                "# schema = StructType([StructField(\"polarity\", StringType(), False), StructField(\"scores\", MapType(StringType(), FloatType()), False)])\r\n",
                "# print(schema)\r\n",
                "\r\n",
                "# schema = StructType([(StructField(\"polarity\", StringType(), False), \r\n",
                "#                      StructField('pos_score', DoubleType(), False), \r\n",
                "#                      StructField('neg_score', DoubleType(), False), \r\n",
                "#                      StructField('neu_score', DoubleType(), False), True)])\r\n",
                "# print(schema)\r\n",
                "\r\n",
                "schema = StructType([StructField(\"polarity\", StringType(), False), StructField(\"scores\", StructType([StructField(\"Positive\", DoubleType(), True), StructField(\"Negative\", DoubleType(), True), StructField(\"Neutral\", DoubleType(), True)]), True)])\r\n",
                "print(schema)\r\n",
                "\r\n",
                "# define the UDF \r\n",
                "ss_udf = sf.udf(sentiment_scores, schema)\r\n",
                "\r\n",
                "# call the udf \r\n",
                "# df_new_model = df_new_model.withColumn('overall_sentiment', ss_udf(df_new_model.model_input_text))\r\n",
                "# df_new_model.show(3)\r\n",
                "\r\n",
                "# call the udf with select (an alternative to withColumn)\r\n",
                "preds_df = df_new_model.select(ss_udf('model_input_text').alias('overall_sentiment'))\r\n",
                "preds_df.show(3)\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "967cb4d6-6d0d-4f3f-a3ba-b237d9fd0a1e",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "a94b4ee96a8b42148f8d55f40a19a6fd"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "StructType(List(StructField(polarity,StringType,false),StructField(scores,StructType(List(StructField(Positive,DoubleType,true),StructField(Negative,DoubleType,true),StructField(Neutral,DoubleType,true))),true)))\n+--------------------+\n|   overall_sentiment|\n+--------------------+\n|[Positive, [0.625...|\n|[Negative, [0.0, ...|\n|[Negative, [0.0, ...|\n+--------------------+\nonly showing top 3 rows"
                }
            ],
            "execution_count": 10
        },
        {
            "cell_type": "code",
            "source": [
                "# select rows\r\n",
                "#\r\n",
                "\r\n",
                "preds_df.printSchema()\r\n",
                "preds_df.show(3)\r\n",
                "preds_df.take(3)"
            ],
            "metadata": {
                "azdata_cell_guid": "53ea95e6-a502-43fc-a3f2-383db1537e6e"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "d943eddbbf024e638883ecefaf2ee518"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "root\n |-- overall_sentiment: struct (nullable = true)\n |    |-- polarity: string (nullable = false)\n |    |-- scores: struct (nullable = true)\n |    |    |-- Positive: double (nullable = true)\n |    |    |-- Negative: double (nullable = true)\n |    |    |-- Neutral: double (nullable = true)\n\n+--------------------+\n|   overall_sentiment|\n+--------------------+\n|[Positive, [0.625...|\n|[Negative, [0.0, ...|\n|[Negative, [0.0, ...|\n+--------------------+\nonly showing top 3 rows\n\n[Row(overall_sentiment=Row(polarity='Positive', scores=Row(Positive=0.625, Negative=0.0, Neutral=0.094))), Row(overall_sentiment=Row(polarity='Negative', scores=Row(Positive=0.0, Negative=1.5, Neutral=0.0))), Row(overall_sentiment=Row(polarity='Negative', scores=Row(Positive=0.0, Negative=0.5, Neutral=0.167)))]"
                }
            ],
            "execution_count": 37
        },
        {
            "cell_type": "code",
            "source": [
                "df_new_model.select('polarity', 'scores').show(2)\r\n",
                "# df_new_model_1 = df_new_model.select('id', 'title', 'url', 'summary', 'search_company', 'polarity', explode('scores').alias(['positive', 'negative', 'neutral']))\r\n",
                "# df_new_model_1.show(3)"
            ],
            "metadata": {
                "azdata_cell_guid": "8d30aa3c-e2c0-4555-8165-f5171e73f567",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "a86f393837f64ed99aae70e44d7f2167"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+--------+-------------------+\n|polarity|             scores|\n+--------+-------------------+\n|Positive|[0.625, 0.0, 0.094]|\n|Negative|    [0.0, 1.5, 0.0]|\n+--------+-------------------+\nonly showing top 2 rows"
                }
            ],
            "execution_count": 20
        },
        {
            "cell_type": "code",
            "source": [
                "# df_new_model_final = df_new_model.select('id', 'title', 'url', 'summary', 'search_company', 'polarity', 'scores.*')\r\n",
                "df_new_model_final = df_new_model.select('id', 'title', 'url', 'summary', 'search_company', 'polarity')"
            ],
            "metadata": {
                "azdata_cell_guid": "469a3636-9a90-4ec4-9115-bc8933d3d4b1"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "6c3074e7b2bb470a805745b9386cd4b5"
                        }
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 22
        },
        {
            "cell_type": "code",
            "source": [
                "df_new_model_final.show(3)"
            ],
            "metadata": {
                "azdata_cell_guid": "4a977a20-8758-4cb4-806e-f35a0369214d"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "da12baa74c614369b3e35b08855a6b6e"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+--------------+--------------------+--------------------+--------------------+--------------------+--------+\n|            id|               title|                 url|             summary|      search_company|polarity|\n+--------------+--------------------+--------------------+--------------------+--------------------+--------+\n|20082425521615|Siemens gets CCI ...|https://www.domai...|Siemens gets CCI ...|C&S ELECTRIC LIMI...|Positive|\n|20082425521616|      ESSAR FORGINGS|                null|Manufacturers & S...|      ESSAR FORGINGS|Negative|\n|20082425521621|Kazikhan Engineer...|                null|Kazikhan Engineer...|KAZIKHAN ENGINEER...|Negative|\n+--------------+--------------------+--------------------+--------------------+--------------------+--------+\nonly showing top 3 rows"
                }
            ],
            "execution_count": 28
        },
        {
            "cell_type": "code",
            "source": [
                "print(df_new_model_final.printSchema())"
            ],
            "metadata": {
                "azdata_cell_guid": "545726e3-c42e-4ae1-957f-42f87f1a2e67"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "889665facc7b4e6db4f1dd7486077f85"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "root\n |-- id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- url: string (nullable = true)\n |-- summary: string (nullable = true)\n |-- search_company: string (nullable = true)\n |-- polarity: string (nullable = true)\n\nNone"
                }
            ],
            "execution_count": 29
        },
        {
            "cell_type": "markdown",
            "source": [
                "Save Data Frame into HDFS as RDD"
            ],
            "metadata": {
                "azdata_cell_guid": "51cb5967-b110-44d8-972c-4abb6d7e8cb4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "df_new_model_final_rdd = df_new_model_final.rdd\r\n",
                "# df_new_model_final_rdd.saveAsTextFile('/COE/news_data/news_rdd/')"
            ],
            "metadata": {
                "azdata_cell_guid": "974f442e-d0a5-4465-ae97-c994b8134144"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "###  Save Data Frame into Hive Table"
            ],
            "metadata": {
                "azdata_cell_guid": "65d2f5b9-6e53-459d-be69-5c63d2ec57ab"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "#df_new_model_final = df_new_model.take(3)\r\n",
                "df_new_model.createOrReplaceTempView('df_new_model_temp')\r\n",
                "df_new_model_final = spark.sql('select polarity from df_new_model_temp')\r\n",
                "df_new_model_final.show()\r\n",
                "# df_new_model_final.write.save('/COE/news_data/sentiment_scores.json', header = True, mode = 'overwrite')"
            ],
            "metadata": {
                "azdata_cell_guid": "83bdf9bd-cdde-4001-8595-1db58830efa9"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "51eb156965fc4a7aabe7e131b94e8b13"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+--------+\n|polarity|\n+--------+\n|Positive|\n|Negative|\n|Negative|\n|Positive|\n|Positive|\n|Positive|\n|Negative|\n+--------+"
                }
            ],
            "execution_count": 59
        },
        {
            "cell_type": "code",
            "source": [
                "df_new_model_final.take()"
            ],
            "metadata": {
                "azdata_cell_guid": "63a70922-3d27-4939-8cd4-8263263a6903"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "a7e5694dfa3f4e3fb56658850fab4097"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "An error was encountered:\n'list' object has no attribute 'take'\nTraceback (most recent call last):\nAttributeError: 'list' object has no attribute 'take'\n\n"
                }
            ],
            "execution_count": 51
        },
        {
            "cell_type": "code",
            "source": [
                "# save dataframe to hdfs\r\n",
                "#\r\n",
                "\r\n",
                "df_new_model_final.write.format('csv').mode('overwrite').option('header', True).save('/COE/news_data/sentiment_model_scores.csv')"
            ],
            "metadata": {
                "azdata_cell_guid": "661480c3-340f-4c12-898e-a0f2b59df07c"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}