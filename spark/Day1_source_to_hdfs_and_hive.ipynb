{
    "metadata": {
        "kernelspec": {
            "name": "pysparkkernel",
            "display_name": "PySpark",
            "language": ""
        },
        "language_info": {
            "name": "pyspark",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "# connection string\r\n",
                "\r\n",
                "servername = \"jdbc:sqlserver://172.31.60.115\"\r\n",
                "db_name = 'EDWSTG'\r\n",
                "username = 'viewuser' \r\n",
                "password = \"view2005user\"\r\n",
                "url = servername + \";\" + \"databaseName=\" + db_name + \";\"\r\n",
                "print(url)"
            ],
            "metadata": {
                "azdata_cell_guid": "5e7cf8cf-5327-4ca6-90e2-e6b8d2388c77",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# to query data from sqlserver \r\n",
                "df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\")\\\r\n",
                "    .option(\"url\", url)\\\r\n",
                "    .option(\"query\", \"select top 100 * from EIPSCM.SCM_H_Purchase_Orders\")\\\r\n",
                "    .option(\"user\", username).option(\"password\", password)\\\r\n",
                "    .load()\r\n",
                "df.printSchema()"
            ],
            "metadata": {
                "azdata_cell_guid": "82c68ad9-c018-4f51-9033-4a1e12ad5edf",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# save as temporary file for querying using sql \r\n",
                "\r\n",
                "df.createOrReplaceTempView('df_temp')\r\n",
                "spark.sql('select * from df_temp limit 2').show() # hiveQL query"
            ],
            "metadata": {
                "azdata_cell_guid": "e1bc319c-df16-4b05-bf00-64a6df920301"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# save data with HDFS\r\n",
                "dtable = 'po_data_sample'\r\n",
                "df.write.format('csv').mode(\"overwrite\").option('header', 'true').save(\"/COE_Demo/\" + dtable)"
            ],
            "metadata": {
                "azdata_cell_guid": "0ce06363-249f-4cd3-b631-c7bfc6ffe45a"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# save pyspark sqldataframe in Hive\r\n",
                "df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)"
            ],
            "metadata": {
                "azdata_cell_guid": "ce45140a-66e7-467a-a3a7-0362b099d3fd"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# querying with hiveQL\r\n",
                "spark.sql('select distinct(HPO_Job_Code) from df_temp limit 10').show()"
            ],
            "metadata": {
                "azdata_cell_guid": "1804e99f-771b-4d06-ad11-d34fa6ac8a01"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}