{
    "metadata": {
        "kernelspec": {
            "name": "pysparkkernel",
            "display_name": "PySpark",
            "language": ""
        },
        "language_info": {
            "name": "pyspark",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## Load data from SQL Server\n",
                "\n",
                "In this notebook we will see how to import pre-selected tables from local SQL Database into HDFS in SQL 2019 BDC using <strong>PySpark.</strong>We also store the data into Hive directory in HDFS so it can be queried using Spark Sql."
            ],
            "metadata": {
                "azdata_cell_guid": "b225d0e2-2284-4dd8-be5d-3452552b327e"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Connection String "
            ],
            "metadata": {
                "azdata_cell_guid": "0a4ab862-6f25-4043-abfe-7874c6a78f2b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "servername = \"jdbc:sqlserver://172.31.60.115\"\r\n",
                "db_name = 'EDWSTG'\r\n",
                "username = 'viewuser' \r\n",
                "password = \"view2005user\"\r\n",
                "url = servername + \";\" + \"databaseName=\" + db_name + \";\"\r\n",
                "print(url)"
            ],
            "metadata": {
                "azdata_cell_guid": "5ce9324f-3a8b-46db-9316-ed05275ca040",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# list of table to read from \r\n",
                "schemas = ['EIPSCM', 'EIPMAS', 'EIPPMP', 'SQLMAS']\r\n",
                "dtable_eipscm = [\"SCM_H_Purchase_Orders\", \"SCM_D_Purchase_Orders\", \"SCM_H_MRN\" , \"SCM_D_MRN\", \"SCM_H_GIN\", \"SCM_D_GIN\", \"SCM_H_Bill_Of_Entry\", \"SCM_D_Bill_Of_Entry_Summary\", 'SCM_H_Offer', 'SCM_D_Offer']\r\n",
                "dtable_eipmas = [\"GEN_M_Jobs\", \"GEN_M_Materials\"]\r\n",
                "dtable_eippmp = [\"GEN_L_MATERIAL_CATEGORY_MATERIAL_GROUP\"]\r\n",
                "dtable_sqlmas = ['GEN_M_Vendors']"
            ],
            "metadata": {
                "azdata_cell_guid": "2c3b0475-a223-4fa5-968b-278c4d643473",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# eipscm schema tables\r\n",
                "\r\n",
                "for dtable in dtable_eipscm:\r\n",
                "    if dtable == 'SCM_H_Purchase_Orders':\r\n",
                "        dtable_wt_schema = schemas[0] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema + \" where HPO_PO_Date between '2019-01-01' and '2020-07-30' and HPO_DS_Code = 3 and HPO_Company_Code = 1\"\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\" + dtable)\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)\r\n",
                "\r\n",
                "    if dtable == \"SCM_D_Purchase_Orders\":\r\n",
                "        dtable_wt_schema = schemas[0] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema + \" where cast(DPO_Inserted_On as DATE) between '2019-01-01' and '2020-07-30'\"\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\"+dtable)\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)\r\n",
                "\r\n",
                "    if dtable == \"SCM_H_MRN\":\r\n",
                "        dtable_wt_schema = schemas[0] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema + \" where cast(HMRN_Inserted_On as DATE) between '2019-01-01' and '2020-07-30'\"\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\"+dtable)\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)\r\n",
                "\r\n",
                "    if dtable == \"SCM_D_MRN\":\r\n",
                "        dtable_wt_schema = schemas[0] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema + \" where cast(DMRN_Inserted_On as DATE) between '2019-01-01' and '2020-07-30'\"\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\"+dtable)\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)\r\n",
                "\r\n",
                "    if dtable == \"SCM_H_GIN\":\r\n",
                "        dtable_wt_schema = schemas[0] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema + \" where cast(HGIN_Inserted_On as DATE) between '2019-01-01' and '2020-07-30'\"\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\"+dtable)\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)\r\n",
                "\r\n",
                "    if dtable == \"SCM_D_GIN\":\r\n",
                "        dtable_wt_schema = schemas[0] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema + \" where cast(DGIN_Inserted_On as DATE) between '2019-01-01' and '2020-07-30'\"\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\"+dtable)\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)\r\n",
                "\r\n",
                "    if dtable == \"SCM_H_Bill_Of_Entry\":\r\n",
                "        dtable_wt_schema = schemas[0] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema + \" where cast(HBOE_Inserted_On as DATE) between '2019-01-01' and '2020-07-30'\"\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\"+dtable)\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)\r\n",
                "\r\n",
                "    if dtable == \"SCM_D_Bill_Of_Entry_Summary\":\r\n",
                "        dtable_wt_schema = schemas[0] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema + \" where cast(DBOES_Inserted_On as DATE) between '2019-01-01' and '2020-07-30'\"\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\"+dtable)\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)\r\n",
                "\r\n",
                "    if dtable == \"SCM_H_Offer\":\r\n",
                "        dtable_wt_schema = schemas[0] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema + \" where cast(HOFFR_Inserted_On as DATE) between '2019-01-01' and '2020-07-30'\"\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\"+dtable)\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)\r\n",
                "\r\n",
                "    if dtable == \"SCM_D_Offer\":\r\n",
                "        dtable_wt_schema = schemas[0] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema + \" where cast(DOFFR_Inserted_On as DATE) between '2019-01-01' and '2020-07-30'\"\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\"+dtable)\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)"
            ],
            "metadata": {
                "azdata_cell_guid": "df610e63-425e-49ef-a93f-bbe4b6c069b0",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# eipmas schema tables\r\n",
                "\r\n",
                "for dtable in dtable_eipmas:\r\n",
                "    if dtable == 'GEN_M_Jobs':\r\n",
                "        dtable_wt_schema = schemas[1] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema + \" where cast(MJOB_Inserted_On as DATE) between '2019-01-01' and '2020-07-30'\"\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\"+dtable)\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)\r\n",
                "\r\n",
                "    if dtable == 'GEN_M_Materials':\r\n",
                "        dtable_wt_schema = schemas[1] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema + \" where cast(MMAT_Inserted_On as DATE) between '2019-01-01' and '2020-07-30'\"\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\"+dtable)\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)"
            ],
            "metadata": {
                "azdata_cell_guid": "881e6da3-dde6-4025-b7d3-d5ad87227452",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# eippmp schema tables\r\n",
                "# ('M001', 'M002', 'M003', 'M004', 'M005', 'M006', 'M007', 'M008', 'M009', 'M010', 'M011' ,'M012', 'M013', 'M014', 'M015')\r\n",
                "for dtable in dtable_eippmp:\r\n",
                "    if dtable == 'GEN_L_MATERIAL_CATEGORY_MATERIAL_GROUP':\r\n",
                "        dtable_wt_schema = schemas[2] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema + \" where LMCMG_Material_Category_Code in ('M001', 'M002', 'M003', 'M004', 'M005', 'M006', 'M007', 'M008', 'M009', 'M010', 'M011' ,'M012', 'M013', 'M014', 'M015')\"\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\"+dtable)\r\n",
                "        df.show()\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)"
            ],
            "metadata": {
                "azdata_cell_guid": "720c8aa0-1994-4123-a728-327c43cdf808",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# sqlmas schema tables\r\n",
                "for dtable in dtable_sqlmas:\r\n",
                "    if dtable == 'GEN_M_Vendors':\r\n",
                "        dtable_wt_schema = schemas[3] + \".\" + dtable\r\n",
                "        query = \"select * from \" + dtable_wt_schema\r\n",
                "        df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\", url).option(\"query\", query).option(\"user\", username).option(\"password\", password).load()\r\n",
                "\r\n",
                "        # load data to HDFS\r\n",
                "        df.write.format('csv').save(\"/COE/\"+dtable)\r\n",
                "        df.show()\r\n",
                "\r\n",
                "        # store hdfs table in hive for later processing\r\n",
                "        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(dtable)"
            ],
            "metadata": {
                "azdata_cell_guid": "4c8be177-fa35-4764-99bf-f7602ee77018"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Query Hive Tables "
            ],
            "metadata": {
                "azdata_cell_guid": "4619275b-0d2d-4e0a-a8bb-8ed4369b9957"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# query hdfs in sql way\r\n",
                "from pyspark import SQLContext\r\n",
                "sqlContext = SQLContext(sc)\r\n",
                "\r\n",
                "# read from hive \r\n",
                "# eipscm\r\n",
                "for dtable in dtable_eipscm: \r\n",
                "    temp = sqlContext.read.parquet('/user/hive/warehouse/' + dtable.lower())\r\n",
                "    temp.createOrReplaceTempView(dtable + \"_pq\" )\r\n",
                "\r\n",
                "    # query using spark sql\r\n",
                "    \r\n",
                "    if dtable.lower() == 'scm_h_purchase_orders':\r\n",
                "        count_rows = sqlContext.sql('select * from ' + dtable.lower() + \"_pq\" + ' where HPO_BA_Code in (\"VCI00929\", \"V0048641\", \"VDN00142\", \"V9000364\", \"VTR00003\", \"VMU00057O\", \"VDA00931\", \"V0041664\", \"VQD00002\")')\r\n",
                "        print(dtable + ':')\r\n",
                "        print(count_rows.count())\r\n",
                "\r\n",
                "        # limit rows\r\n",
                "        count_rows.createOrReplaceTempView(dtable.lower() + '_1000')\r\n",
                "    \r\n",
                "    else:\r\n",
                "        count_rows = sqlContext.sql('select * from ' + dtable.lower() + \"_pq\")\r\n",
                "        print(dtable + ':')\r\n",
                "        print(count_rows.count())\r\n",
                "\r\n",
                "        # limit rows\r\n",
                "        count_rows.createOrReplaceTempView(dtable.lower() + '_1000')\r\n",
                "\r\n",
                "# eipmas\r\n",
                "for dtable in dtable_eipmas: \r\n",
                "    temp = sqlContext.read.parquet('/user/hive/warehouse/' + dtable.lower())\r\n",
                "    temp.createOrReplaceTempView(dtable + \"_pq\" )\r\n",
                "\r\n",
                "    # query using spark sql\r\n",
                "    count_rows = sqlContext.sql('select * from ' + dtable.lower() + \"_pq\")\r\n",
                "    print(dtable + ':')\r\n",
                "    print(count_rows.count())\r\n",
                "\r\n",
                "    # limit rows\r\n",
                "    count_rows.createOrReplaceTempView(dtable.lower() + '_1000')\r\n",
                "\r\n",
                "# eippmp\r\n",
                "for dtable in dtable_eippmp: \r\n",
                "    temp = sqlContext.read.parquet('/user/hive/warehouse/' + dtable.lower())\r\n",
                "    temp.createOrReplaceTempView(dtable + \"_pq\" )\r\n",
                "\r\n",
                "    # query using spark sql\r\n",
                "    count_rows = sqlContext.sql('select * from ' + dtable.lower() + \"_pq\")\r\n",
                "    print(dtable + ':')\r\n",
                "    print(count_rows.show())\r\n",
                "\r\n",
                "    # limit rows\r\n",
                "    count_rows.createOrReplaceTempView(dtable.lower() + '_1000')\r\n",
                "\r\n",
                "# sqlmas\r\n",
                "for dtable in dtable_sqlmas:\r\n",
                "    temp = sqlContext.read.parquet('/user/hive/warehouse/' + dtable.lower())\r\n",
                "    temp.createOrReplaceTempView(dtable + \"_pq\" )\r\n",
                "\r\n",
                "    # query using spark sql\r\n",
                "    count_rows = sqlContext.sql('select * from ' + dtable.lower() + \"_pq\")\r\n",
                "    print(dtable + ':')\r\n",
                "    print(count_rows.show())\r\n",
                "\r\n",
                "    # limit rows\r\n",
                "    count_rows.createOrReplaceTempView(dtable.lower() + '_1000')"
            ],
            "metadata": {
                "azdata_cell_guid": "25f18dac-92a1-44c2-84fe-9b2bcd0d0bfd"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}