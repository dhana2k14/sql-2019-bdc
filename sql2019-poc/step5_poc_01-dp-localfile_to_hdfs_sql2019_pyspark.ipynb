{
    "metadata": {
        "kernelspec": {
            "name": "pysparkkernel",
            "display_name": "PySpark",
            "language": ""
        },
        "language_info": {
            "name": "pyspark",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Sentiment Analysis on News Articles of Vendors\n",
                "\n",
                "## Dealing with Semi-Structured Data stored in HDFS of SQL 2019\n",
                "\n",
                "- In this notebook we will understand how to process, transform, prepare a JSON file data which can be further used for tasks like ML model trainings. Since we already developed the underlying ML model using external environment we consume the model within SQL 2019 BDC for predictions. \n",
                "- The model end point is hosted in Azure Kubenetes Cluster (ACS) for batch and live model inferences."
            ],
            "metadata": {
                "azdata_cell_guid": "64ae59da-5db1-4a42-90a3-b863568810d4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql import SparkSession\r\n",
                "spark = SparkSession\\\r\n",
                "        .builder\\\r\n",
                "        .appName(\"Spark_Ingestion_Job\")\\\r\n",
                "        .config(\"spark.executor.memory\", \"20g\")\\\r\n",
                "        .config(\"spark.executor.instances\", \"3\")\\\r\n",
                "        .config(\"spark.master\", \"yarn\")\\\r\n",
                "        .config(\"spark.submit.deployMode\", \"client\")\\\r\n",
                "        .config(\"spark.driver.memory\", \"30g\")\\\r\n",
                "        .enableHiveSupport()\\\r\n",
                "        .getOrCreate()"
            ],
            "metadata": {
                "azdata_cell_guid": "47ad57d0-519f-4940-a902-b4e14f1fa588",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "###  Loading Data\n",
                "\n",
                "We can use import files option from the HDFS directory where we want to store the data to import local files into HDFS since there is no programmable way."
            ],
            "metadata": {
                "azdata_cell_guid": "5334480b-7cb3-4ff7-94aa-95dba634f381"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# do with RDD\r\n",
                "import json\r\n",
                "news_data_rdd = sc.textFile('/COE/news_data/contify_insights_new.json').map(json.loads)\r\n",
                "news_data_rdd.take(1)"
            ],
            "metadata": {
                "azdata_cell_guid": "595cff00-1a3a-4ddb-8b76-b57bb0bec62c"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "news_data_rdd.count()"
            ],
            "metadata": {
                "azdata_cell_guid": "757b157d-4acf-423c-91de-f3e01a8b4607"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql.types import Row\r\n",
                "import pyspark.sql.functions as sf \r\n",
                "import requests\r\n",
                "\r\n",
                "def spliter(lines):\r\n",
                "    data = {}\r\n",
                "    line = lines['results']\r\n",
                "    if line:\r\n",
                "        for d in line:\r\n",
                "            data['id'] = d['id']\r\n",
                "            data['title'] = d['title']\r\n",
                "            data['summary'] = d['summary']\r\n",
                "    else:    \r\n",
                "        data['id'] = ''\r\n",
                "        data['title'] = ''\r\n",
                "        data['summary'] = ''\r\n",
                "    data['search_company'] = lines['search_company']\r\n",
                "    return data\r\n",
                "\r\n",
                "rdd_df = news_data_rdd.map(lambda x: Row(**spliter(x)))\r\n",
                "rdd_df.collect()"
            ],
            "metadata": {
                "azdata_cell_guid": "c36e7381-2e93-4299-a12f-af994be9a113",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "df = rdd_df.toDF()\r\n",
                "df.printSchema"
            ],
            "metadata": {
                "azdata_cell_guid": "5db07bd4-7f36-4488-871d-17aef7feb8c5"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "type(df)"
            ],
            "metadata": {
                "azdata_cell_guid": "d3d06e57-74b8-4a1d-aff0-b36dde16b2b9"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# convert pyspark dataframe to pandas dataframe\r\n",
                "#\r\n",
                "pd_df = df.toPandas()\r\n",
                "# pd_df = pd_df[pd_df['summary'] != '']\r\n",
                "# pd_df.reset_index(inplace = True)\r\n",
                "# pd_df.drop('index', axis = 1, inplace = True)"
            ],
            "metadata": {
                "azdata_cell_guid": "ac4f4a02-e86a-4c66-8c8e-8c0093ad5068"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# model scoring \r\n",
                "def sentiment_scores(text_input):\r\n",
                "    response = requests.post(\"http://52.187.124.32:80/api/v1/service/absa-sentiment-predictor-v2/score\", text_input, headers = {'Content-Type' : 'application/json', 'Authorization': 'Bearer 1Q7d5p2SqViNlQbhe6gtHBAiZ5MB58rU'})\r\n",
                "    response = response.json()\r\n",
                "    polarity = response['_doc_polarity']\r\n",
                "    scores = response['scores']\r\n",
                "    return(polarity, scores)\r\n",
                "\r\n",
                "# attach model results to dataframe\r\n",
                "def model_scores(dataframe):\r\n",
                "    for index, row in dataframe.iterrows():\r\n",
                "        pol, scores = sentiment_scores(row['scoring_text'].encode('utf-8'))\r\n",
                "        dataframe.loc[index, 'polarity'] = pol\r\n",
                "        dataframe.loc[index, 'positive'] = str(list(filter(None, [v if k == 'Positive' else 0 for k, v in scores.items()])))\r\n",
                "        dataframe.loc[index, 'neutral'] = str(list(filter(None, [v if k == 'Neutral' else 0 for k, v in scores.items()])))\r\n",
                "        dataframe.loc[index, 'negative'] = str(list(filter(None, [v if k == 'Negative' else 0 for k, v in scores.items()])))\r\n",
                "    return dataframe"
            ],
            "metadata": {
                "azdata_cell_guid": "e545ed8f-8d64-45cc-9623-c3ea8db058ce"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# text pre-processing\r\n",
                "#\r\n",
                "pd_df['scoring_text'] = '{\"news\":\"' + pd_df['summary'] + '\",' + '\"name\":\"' + pd_df['search_company'] + '\"}'\r\n",
                "pd_df\r\n",
                "\r\n",
                "# application of model scoring \r\n",
                "#\r\n",
                "model_score_df = model_scores(pd_df)\r\n",
                "model_score_df[['polarity', 'positive', 'negative', 'neutral']].head() # print top 5 results"
            ],
            "metadata": {
                "azdata_cell_guid": "68431f7f-8cf5-4f12-a843-6c7d73cc5ea9",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# convert pandas dataframe to Pyspark dataframe\r\n",
                "#\r\n",
                "model_scores_spark_df = spark.createDataFrame(model_score_df)\r\n",
                "print(type(model_scores_spark_df))\r\n",
                "print(model_scores_spark_df.printSchema())\r\n",
                "print(model_scores_spark_df.show(5))"
            ],
            "metadata": {
                "azdata_cell_guid": "097a91e0-4e71-4416-9440-550794a05c06"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# save spark dataframe to hdfs \r\n",
                "#\r\n",
                "model_scores_spark_df.write.format('csv').mode('overwrite').option('header', True).save('/COE/news_data/news_rdd/sentiment_scores.csv')"
            ],
            "metadata": {
                "azdata_cell_guid": "7426b7f6-00f2-4dff-9d4e-3df560ff718f"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}